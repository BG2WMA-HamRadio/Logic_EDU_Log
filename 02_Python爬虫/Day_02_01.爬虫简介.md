## 爬虫简介
### 爬虫的定义
- 繁体中文版的维基上对爬虫的定义稍稍有点过时了，其内容主要是针对搜索引擎的Spider定义的：
  - 网络爬虫（Web Crawler），也叫网络蜘蛛（Spider），是一种用来**自动**浏览万维网的网络机器人。其目的一般是编纂**网络索引**。
- 网络爬虫的大致理解：用以代替人去模拟浏览器进行网络操作的一个程序或者一段代码。

### 为什么需要爬虫
- 为其他程序提供数据源
  - 搜索引擎的网络索引
  - 数据分析和机器学习的数据源。
  
### 获取数据的方式
- 自有的私有数据
- 第三方购买
- 使用爬虫爬取的数据

### Python 用作爬虫的优势
- 相对于PHP, Python在多线程异步方面具有强大优势。
- 相对于C/C++/Java来讲，Python编程简单，支持模块多。
- Python具有Scrype框架，开发效率高。


### 爬虫的分类
- 通用网络爬虫：多指搜索引擎使用的爬虫。
- 聚焦网络爬虫：对既定目标有选择地抓取数据
- 增量式网络爬虫：对现有数据进行增量式的更新，不用每次将所有数据全部下载，只抓取新产生的或者发生变化的数据。
- 深层网络爬虫：大部分内容不能通过静态连接获取，隐藏在表单后的，需要用户提交一些关键词才能获得的web页面中的数据。比如需要用户登陆后才能显示的页面中的数据。


### 几个概念
#### GET 和 POST
- GET 和 POST 都需要大写
- GET：查询参数会在URL上显示出来。
  - 例如
  ```
  https://www.google.com/search?q=%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB&oq=%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB&aqs=chrome..69i57j69i61l3j0l3.2739j0j8&sourceid=chrome&ie=UTF-8
  ```
- POST：查询参数需要提交的数据是隐藏在From表单中的，不会在URL地址中显示出来。例如大多数登录页面或者个人信息修改页面等。


#### URL（Uniform REsource Locator，统一资源定位符）
- 统一资源定位符的结构：
  - 协议：`http://`或者`https://`
  - 所访问的主机地址：例如`www.google.com`
  - 端口号，如果没有特殊指定，则使用默认值`http: 80`和`https: 443`
  - 资源路径：指资源在服务器中的相对路径，就是所访问的网页文件在网站服务器根目录的相对地址。例如`/telephone/iphone/iphonex.html`
  - #anchor, 锚点：在前端网页作为页面定位使用。例如`https://music.163.com/#/friend`
  
  - 注意：在浏览器请求一个URL，浏览器会对这个URL进行编码，当显示非ASCII字符的时候，会编码成`%`+ 一个十六进制数字的形式。
  - 比如上例中的`%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB&oq`解码后即为`网络爬虫`。

#### User-Agent 用户代理
- 主要用途是记录用户的**浏览器**和**操作系统**等状态，以使用户获得更好的HTML体验。

#### Refer
- 用来记录用户的来源网站，很多时候可以利用Referer来反爬虫。

#### 状态码
- 在前述文章中有过介绍，这里详细阐述几个跟爬虫相关的状态码：
  - 200：请求成功
  - 301：永久重定向，比如将某个弃用域名重定向到现有域名。
  - 302：临时重定向，比如临时重定向到登录页面，用户登录成功后返回原网页。
  - 404：最经常见到的网络错误页面：没有找到（输入了错误的网址或者网址已经删除、丢失等）
  - 500：服务器内部请求。
  
### 抓包工具
- 首先使用的是Chrome和Firefox所自带的网页分析工具，其中比较有用途的项目为：
  - Elements：网页元素，用来提取数据和分析数据，需要注意的是，这里提取到的数据并**不一定是准确**的。
  - Console：带有命令提示符的控制台，用来执行一些分析命令。
  - Sources: 信息来源，整个网站加载的文件都可以在这里找到。但是需要注意的是，这里显示的网页源代码并**不一定**能够带有Elements中显示的数据。
  - NetWork：网络（信息抓包），能够看到网络动作中的请求等其他信息。
  
- 网络抓包工具：
  - 最著名（且免费）的网络抓包工具非**Wireshark**不可了。
  
  
